* fix lifetime issue requiring sync in cuda_allocCopySentenceVariationIndices(

* incompatible_sources: could be CRefs?

* move buildSourceListMap to c++?  save ~3s + unwrap time (4s?) from PreCompute.
  * gonna require moving ClueManger.knownSourceMaps data to c++

* investigate loadClues time. It's going to get worse I bet.
  * -1s by removing extra unnecessary compatibilty check in initSrcBits.
  * we're currently loading, then, validating, then initialize src bits.
    what if we loaded, and initialized src bits in the validator? wouldn't
    that make validation faster?
  * If I had to guess i'd say Validator.isValidResult is a bottleneck here
    (profiling just loadClues could probably confirm this). It calls a function
    that calls _.uniqBy(), a known very slow function.
  * Even just replacing that with populating a Set() and comparing to expected
    size I bet would chop off a significant chunk of time.
  * But if we're going to do that, why not just build the set dynamically as
    we build the expected result?
  * And then, if we're goint to build a set, why not just populate a bitfield?

* can i sort sources within each device_sources list, in order of the smallest
  xor_sources IndexSpanPair they will test against? like, could I store
  candidates in an ordered set, with a test for the smallest IndexSpanPair
  combined size in the comparator function? or, is a candidate a sourcelist
  already? or just a single source?

* what does "ready: 0" mean for filter? I'm calling the filter what seems like
  an unnecessary amount of times with ready:0 when running -c15,15 -x2

* have a stream keep track of how many times it's been run, and/or a total
  count of the number of indices it's processed, then total all of them at
  the end, so i show: num streams executed (M), num indices processed (N)

**************************************************************************
* how about I actually compare new code results with old code results?
  like, cuda-xor. but I need to remove the xorSourceList wrapping from that.
  once that's done, I can remove a big paragraph below about wrapping.
**************************************************************************

* consider stripping some types out of combo_maker.h into separate files.
* PreComputeData probably needs to go do it's own thing. Some form of singleton.
* .cu dependencies

**************************************************************************
* i have count(), countIndices(), and sum_sizes() and I think they all do 
  the same thing or close to it.  maybe a util.h?
  sum_vector_sizes()/sum_container_sizes()?
  add vec_product, vec_to_string, etc.
**************************************************************************
  
* markAllCompatibleOrSources in c++. is this marking used?

* get rid of std::async? have 1 thread waiting on queue, executing filters,
  and main thread just adds execute requests to queue?
  * async, while a sloppy approach, does appear to be reasonably performant.
    so, the goal here would be to play with more advanced thread management
    such as a single worker thread and a locking queue, and maybe some other
    synchronization primitives, just for the sake of using them. performance
    may or may not be any better.

* Optimizing merge/get_compat_combos. 200M combos currently, and no streams.
  Low reward atm, but eventually might make sense.
  * 200M kernel finishes in ~35ms, which DOES NOT give us enough time to do
    all the things we want, when interleaving streams:
    * copy results from device (~10ms)
    * iterate results and append indices to "hit indices" list. (~135ms)
    * mayyybe do some source-merging on CPU. not sure. very possibly though,
      given that the #of hits is a overall a very small (??ms)
  * it seems like a decent sized chunk of device memory to alloc/copy, esp
    if there are multiple streams (x2, x3 that amount).


* WHEN MERGE EXCEEDS UINT64_T: when I get to this point, which will likely
  result from too many --xor args being specified (although could also happen
  from too many sources of a fewer # of --xor args), I can break down the set
  of --xor args into separate chunks, incremented separately (with separate
  indices). for example, I have an array of 8 matrices, I can make that two
  arrays of 4 matrices. the first array will be 0,0,0,0 for a long time, and
  I can just iterate on the 2nd array until it fills up, then increment first
  array to 0,0,0,1 and process second again. given that I will probably end up
  "chunking" this for kernels anyway due to result set size, I could probably
  chunk-run until array 2 reaches capacity, then "increment" array 1 indices
  on CPU, copy them back over to GPU, and run more chunks. It's not *clear*
  that having an all-GPU solution would be a big win, in other words, due to
  result-set size limitations.

* running multiple list_pair_compat kernels in merge stage seems unnecessary;
  i could make that kernel more loopy and handle all list pairs in one call.
  low priority though, as it finishes the current impl in under 25ms.

* HACK: 35 passed to buildKnownNcSourceListMap, in cm-precompute.ts.
  The problem here is that we have no defined upper-bound on how many primary
  sources a single "clue" or "useSource" could represent. 21 for example,
  which is a rather long one (san juan hill buffalo soldier), goes up to
  30 currently.
  When we call buildAllUseNcDataLists("21"), it is (apparently) unbounded -
  it'll find all occurances of 21.
  When we call buildKnownNcSourceListMap, we could also be unbounded, but
  that would require knowing the longest variation for each sentence, and
  adding them up, I think.
  For practical purposes, 35 seems like a reasonable stopping point at this time.
  However, we really should enforce this in buildAllUseNcDataLists(), otherwise
  we're going to hit the bug again when sourceListMap and allXorNcDataLists
  aren't in sync.

* why am I passing primaryNameSrcList and ncList of XorSources back to node?
  Sure I use them to generate sourceBits and usedSources, but are they used
  for anything else? I'm probably merging them and shit, but why, what do I
  do with it all when I'm done? I have a feeling, that I am passing the NcLists
  to Native.addCandidate() as well, from where it is *ONLY USED TO GENERATE
  SOURCEBITS* on the add-on side (I think). So is the *ONLY* reason I am
  maintaining ncLists on the node side, is to use them as the "wire protocol"
  for Node<->Addon communication of sourceBits!?
  * Note that this wrap-back-to-JS is disabled now. As a result -t --fast is
    broken, but it's better to fix that another way (or convert to c++) than
    deal with the insanely long wrap times.
  * Note also that since I'm not passing back to JS, the necessity of all the
    ncList/primaryNameSrcList stuff on the C++ side comes into question. I
    suspect it may be needed for something, but I haven't actually confirmed.
    I do know that having to convert a SourceList to a SourceCompatibilityList
    prior to copying to device memory is a consequence I'd like to get rid of.

* i'm not really sure what the point of having a whole separate confederate
  variation is. it seems unnecessary, a performance pessimization, and in
  general problematic.

* tricky situation that should be fixed (and in general, beyond this specific
  tricky instance). blue is a synonym of navy in clues 1.  we later go on to
  derive "navy" as a solution to "dark,blue".  but if the source of the "blue"
  in this case is the synonym of navy, it should not be allowed.

  to achieve this, it feels like we'll have to (optionally? for components.show)
  attach the "source/origin component" to variation sources. e.g. "navy" gets
  attached to that primary synonym "blue", somehow. when --add'ing, check and
  fail if --add'ed word matches any primary "origin" words.

* I may have figured out what that NO_DUPLICATES thing in peco was all
  about. in cm-precompute.ts fillKnownNcSourceListMapForSum, with sum=2
  we apparently iterate over all 1,1 combinations, including duplicates
  like 'newton:1','newton:1' currently. probably should disable that?
  (it's a tiny fraction of combos so probably not a big deal)
  Haha. This may be what's breaking things right now.

* additional thoughts on optimizaiton:
 - only call addCandidate every 1000+ candidates. maintain a list and
   bulk push them.
 - there's some "i didnt do this coz it's 18s out of 18m" comment in
   combo-maker.ts that I could look into. related to using CountBits
   I think.
 - also copyOnMerge logic in mergeAllSources might be worthwhile to
   look into

* candidates.comboListMap should be a vector. It's not even strictly
  necessary anymore.

* NCData/List -> NcListContainer

* output max variation count and max source count (per sentence) at load.
* fail if max sources > 127 for now

* usesourcebits -> implement andCompatible
* eventually bits per sentence could be based on max sources per sentence
  (uniqueNameCount) and usedSources bitset size is the sum, rather than
  being a fixed size (128*9)

* add syn/hom of anagrams at .. post-process?
* do i need a new variation category, "prefixes", for which we also do
  a syn/hom lookup in post-process? e.g. cogburn -> cog -> tooth.
  the alternative is that "cog" is ... what.. an anagram? and thus we
  do a syn lookup? maybe "anagram" could be generalized? kinda nice to
  have each "rule" encapsulated as a separate variation entry type.

* -t full support, including --add, will be necessary (might be working now)

* "addon-mode" module return string "Debug" or "Release" for setting addon path

* so there is a problem with "ignore":true clues which depend on clues in "sentences"
  that don't support the ignore flag. this problem will go away when all clues
  are converted to sentences, presumably, but it's giving incorrect results
  as it stands (hobbit,home appear in valid combinations with xor.req,
  for example, when they should be "used" by "eye").

* -t 20,s or 24,s are valid, shouldn't be. looks like restrictSameClueNumber
  is broke.
  * now it crashes. after putting in support for non-unique legacy clue names
    into the uniquePrimaryClueNames list.
  * so one idea is we could justa call Validator.validateSources(validateAll: false)
    for any supplyed -t (or --xor, --or, etc) combo. If it doesn't pass
    validator, then, duh, it's not valid. The problem with this is if I want
    to support name:count syntax: Validator doesn't (currently) support it.
  * Maybe the crash on bad combos is something I can live with for now.

* NameCount.listHasCompatibleSources looks unnecessary, or weird, or both

* so, ClueManager.getClueList(1) is the culprit. In every case, I need
  to review what I am doing with the list when I call it with 1.

* All of the "getNumPrimarySources()" stuff is a bit weird now isn't it
  I should look into all uses and confirm what it means. And consider
  the "MaxPossiblePrimarySources" case as a possible solution (largest
  sourcelist variation from each sentence).

* validateSources(name) -- enforce everywhere, for (2) below

validatesources is apparently happy with old:3=not:1,not:1,old:1
which is wrong because:
1) FIXED: two nots from two different variations of sentence three. i though i checked
   for and prevented this with the "candidates" thing somewhere
2) TODO: this is like that weird case with "bird" being the "name" of a clue that had
   "bird" as a source, and that should be disallowed.
   
clues3.json:  { "name": "old",             "src": "new,not" },


* OPTIMIZITIONS:

* adding "powerful" to sentence3 components list really caused the number
  of combinations to explode. It's funny that it didn't actually change the
  output, but that's probably due to the xor.req's I used. There is something
  to investigate here and a couple of potential fixes for this:
  * Investigate: am I ensuring that the sentence words + all components
     form a **unique** list? Should be in Sentence.getUniqueComponentNames
     or something.
  * I think I can do a 2nd pass of index filtering. Iirc, the order of
     filtering matters, and while I currently do something like "remove items
     from list B that are incompatible with items in list A", I don't do
     the inverse, or something like that, which may help.
     NOTE: I don't think filtering matters right now . It only affects native
     peco-loop timing which is currently fast.
     NOTE: I've looked at this and I think I'm testing every combo, nothing
     further to optimize.
  * I think there is potential for a filtering step in here somewhere, where,
     for example, I look at all of the NameSrc results for a particular useNC,
     and see what they have in common. I'm just spitballing, but something
     like "well all NcDataCombinations for this NC have *this* and *that*
     source, so lets leave those sourceBits in for the compatibility check,
     but eliminate the words from the combinatorial set, or something.

     - Eh. I don't think i'm on to anything important there either. I mean,
     you either filter *as* you're considering sources to combine, or you
     filter during the combining process *after* you build lists of prospective
     sources to combine (or both). I am not convinced there is anything here
     but probably worth thinking about more.

     - after some more thought, i do think this is worth exploring. probably
     i'll be screwed without some GPU processing, at which point any data
     optimizations will be kind of moot. we'll find out once all clues are
     converted to sentences.

* I think opportunity for optimization is running out in mergecompatible. 
  One last thing to try is somehow having mergecompatLists (and perhaps
  getMergeData) take either a MergedSourcesList or a Source.List. Then
  we'd need a variation of mergeSources took either as the first arg as well.

* so If I really want to speed up combo-maker further, probably I should
  port the whole getCombosForUseNcLists function and dependences (first/next)
  to C++. I don't think just porting mergeCompatibleSources3 and dependencies
  alone will do much as each call is short so the overhead of the call isn't
  worth it, but there are a million (literally) calls.

** the below was a pre-cuda optimization idea
* from notepad:  on elimination: this should only be relevant to combo-making.
  once a component word is chosen for a particular sentence, all other
  sententce combinations that do not contain that component word should be
  eliminated.
  "So I'll need to keep copies of the per-sentence combination list (list of
  string sets - the sets themselves need not be copied), so that I can remove
  the incompatible combinations on a per-generated-combo basis."
  * this is a potential real optimization I think, for combo-making. Eliminate
    compares altogether vs. elimination via compare.
  * it seems this may be relevant for --xor validation and precompute as well?
    * DONE: should auto-fail if conflicting --xor's are provided
    * what about --or's? do we need to prune --xor's in that case?)
  * need to iron it out more detail above and implement it.
