* -t wrap is very slow for simple combinations, e.g. -t navy,'man o war' = 43 seconds.
  the solution to this would seem to be to move the relevant parts of show-components
  to C++ in order to avoid wrapping.
  * getCountListArrays - easyish
    * DONE need to move knownSourceMap.clues to C++. Might require defining clue type in c++.
    * DONE then show-components::add_result_for_sources should be implementable.
    * DONE i also wonder what I am doing exactly for get_compound_nc_sources; currently I
      am just calling cm::get_nc_sources, and that may be fine, but the JS code for
      compound case was using ClueManager.clueList or something, which is weird.
      * I wonder if that "clues" field in knownSourceMap can cover this case, so I can
        compare results?
  * DONE showCountListArrays - easy

  * TODO addRemoveOrReject - probably could wrap only the contents of this set back to
    JS and perform the action there.

* i should probably document and/or name and/or encapsulate some of the clue-manager
  global data into classes to help me remember what is what and what goes where. its
  a bit of a mess of rather poorly named global data and functions at the moment,
  and i don't have a lot of intuition about it all, so could use the help of some
  additional structure.

* node clues -pf.72 -t buck
  * tentatively fixed.
  
* I think there are a couple places where I'm not using or_src.xor_compat:
  * somewhere in JS, which is commented, but maybe stale code now.
  * in the filter kernel somewhere
  * oddly though, sometimes it only produces results when I call markAll, even
    though I can't identify where it's actually used.
  * why don't i change the field name to is_xor_compat, and see what breaks.
  
* getUniqueClueNameCount -> Native.get_num_unique_clue_names

* hash/is_equal for NameCount, make (nc)knownSrcListMap key a NameCount

* reverse-validate components, syns, ans, homs, make sure the keys of these objects
  actually exist in one of the variations? After the variations have been generated,
  in case they're a syn/an/hom of a component variation, for example.

* merge_support: uint64_t num_combos{200'000'000}; // hard-coded chunk size
   should probably (eventually) figure out:
   - hardcoded:
   - GD = goal duration (ms) per chunk (e.g. 50ms)
   - CS = combos per SM that can be done in GD (e.g. 20M)
   - MB = max bytes, to limit on-device memory (e.g. 1GB) 
   - calculated:
   - MC = max combos that can be done in GD: (CS * num_sm), (e.g. 200M for 10sm, 1B for 50sm)
   * I may also want to eventually divide MB by StreamCount and use multiple streams.
* Optimizing merge/get_compat_combos. 200M combos currently, and no streams.
  Low reward atm, but eventually might make sense.
  * 200M kernel finishes in ~35ms, which DOES NOT give us enough time to do
    all the things we want, when interleaving streams:
    * copy results from device (~10ms)
    * iterate results and append indices to "hit indices" list. (~135ms)
    * mayyybe do some source-merging on CPU. not sure. very possibly though,
      given that the #of hits is a overall a very small (??ms)
  * it seems like a decent sized chunk of device memory to alloc/copy, esp
    if there are multiple streams (x2, x3 that amount).

* synonyms need to be global. probably all of them. global hash map.
  this will reduce load time and merge time. not 100% sure how to go about
  implementing though. but should figure it out.

* no --xor == no filter call?  it's crashing due to empty xor data probably
  (but probably shouldn't even be called)
  what about --or with no --xor? bet that's fun. probably should fail (or fix).

* fix lifetime issue requiring sync in cuda_allocCopySentenceVariationIndices(
  * store in MFD

* incompatible_sources: could be CRefs?

* can i sort sources within each device_sources list, in order of the smallest
  xor_sources IndexSpanPair they will test against? like, could I store
  candidates in an ordered set, with a test for the smallest IndexSpanPair
  combined size in the comparator function? or, is a candidate a sourcelist
  already? or just a single source?

* what does "ready: 0" mean for filter? I'm calling the filter what seems like
  an unnecessary amount of times with ready:0 when running -c15,15 -x2

* have a stream keep track of how many times it's been run, and/or a total
  count of the number of indices it's processed, then total all of them at
  the end, so i show: num streams executed (M), num indices processed (N)

**************************************************************************
* how about I actually compare new code results with old code results?
  like, cuda-xor. but I need to remove the xorSourceList wrapping from that.
  once that's done, I can remove a big paragraph below about wrapping.
**************************************************************************

* consider stripping some types out of combo_maker.h into separate files.

**************************************************************************
* i have count(), countIndices(), and sum_sizes() and I think they all do 
  the same thing or close to it.  maybe a util.h?
  sum_vector_sizes()/sum_container_sizes()?
  add vec_product, vec_to_string, etc.
**************************************************************************
  
* get rid of std::async? have 1 thread waiting on queue, executing filters,
  and main thread just adds execute requests to queue?
  * async, while a sloppy approach, does appear to be reasonably performant.
    so, the goal here would be to play with more advanced thread management
    such as a single worker thread and a locking queue, and maybe some other
    synchronization primitives, just for the sake of using them. performance
    may or may not be any better.

* WHEN MERGE EXCEEDS UINT64_T: when I get to this point, which will likely
  result from too many --xor args being specified (although could also happen
  from too many sources of a fewer # of --xor args), I can break down the set
  of --xor args into separate chunks, incremented separately (with separate
  indices). for example, I have an array of 8 matrices, I can make that two
  arrays of 4 matrices. the first array will be 0,0,0,0 for a long time, and
  I can just iterate on the 2nd array until it fills up, then increment first
  array to 0,0,0,1 and process second again. given that I will probably end up
  "chunking" this for kernels anyway due to result set size, I could probably
  chunk-run until array 2 reaches capacity, then "increment" array 1 indices
  on CPU, copy them back over to GPU, and run more chunks. It's not *clear*
  that having an all-GPU solution would be a big win, in other words, due to
  result-set size limitations.

* running multiple list_pair_compat kernels in merge stage seems unnecessary;
  i could make that kernel more loopy and handle all list pairs in one call.
  low priority though, as it finishes the current impl in under 25ms.

* tricky situation that should be fixed (and in general, beyond this specific
  tricky instance). blue is a synonym of navy in clues 1.  we later go on to
  derive "navy" as a solution to "dark,blue".  but if the source of the "blue"
  in this case is the synonym of navy, it should not be allowed.

  to achieve this, it feels like we'll have to (optionally? for components.show)
  attach the "source/origin component" to variation sources. e.g. "navy" gets
  attached to that primary synonym "blue", somehow. when --add'ing, check and
  fail if --add'ed word matches any primary "origin" words.

* similarly, validateSources(name) -- enforce everywhere, for below
  validatesources is apparently happy with old:3=not:1,not:1,old:1
  which is wrong because:
   this is like that weird case with "bird" being the "name" of a clue that had
   "bird" as a source, and that should be disallowed.
   clues3.json:  { "name": "old",             "src": "new,not" },

* I may have figured out what that NO_DUPLICATES thing in peco was all
  about. in cm-precompute.ts fillKnownNcSourceListMapForSum, with sum=2
  we apparently iterate over all 1,1 combinations, including duplicates
  like 'newton:1','newton:1' currently. probably should disable that?
  (it's a tiny fraction of combos so probably not a big deal)
  Haha. This may be what's breaking things right now.

* additional thoughts on optimizaiton:
 - only call addCandidate every 1000+ candidates. maintain a list and
   bulk push them.

* candidates.comboListMap should be a vector. It's not even strictly
  necessary anymore.

* NCData/List -> NcListContainer

* eventually bits per sentence could be based on max sources per sentence
  (uniqueNameCount) and usedSources bitset size is the sum, rather than
  being a fixed size (128*9)

* add syn/hom of anagrams at .. post-process?
* do i need a new variation category, "prefixes", for which we also do
  a syn/hom lookup in post-process? e.g. cogburn -> cog -> tooth.
  the alternative is that "cog" is ... what.. an anagram? and thus we
  do a syn lookup? maybe "anagram" could be generalized? kinda nice to
  have each "rule" encapsulated as a separate variation entry type.

* NameCount.listHasCompatibleSources looks unnecessary, or weird, or both


* OPTIMIZITIONS:

* adding "powerful" to sentence3 components list really caused the number
  of combinations to explode. It's funny that it didn't actually change the
  output, but that's probably due to the xor.req's I used. There is something
  to investigate here and a couple of potential fixes for this:
  * Investigate: am I ensuring that the sentence words + all components
     form a **unique** list? Should be in Sentence.getUniqueComponentNames
     or something.
  * I think I can do a 2nd pass of index filtering. Iirc, the order of
     filtering matters, and while I currently do something like "remove items
     from list B that are incompatible with items in list A", I don't do
     the inverse, or something like that, which may help.
     NOTE: I don't think filtering matters right now . It only affects native
     peco-loop timing which is currently fast.
     NOTE: I've looked at this and I think I'm testing every combo, nothing
     further to optimize.
  * I think there is potential for a filtering step in here somewhere, where,
     for example, I look at all of the NameSrc results for a particular useNC,
     and see what they have in common. I'm just spitballing, but something
     like "well all NcDataCombinations for this NC have *this* and *that*
     source, so lets leave those sourceBits in for the compatibility check,
     but eliminate the words from the combinatorial set, or something.

     - Eh. I don't think i'm on to anything important there either. I mean,
     you either filter *as* you're considering sources to combine, or you
     filter during the combining process *after* you build lists of prospective
     sources to combine (or both). I am not convinced there is anything here
     but probably worth thinking about more.

     - after some more thought, i do think this is worth exploring. probably
     i'll be screwed without some GPU processing, at which point any data
     optimizations will be kind of moot. we'll find out once all clues are
     converted to sentences.

** the below was a pre-cuda optimization idea
* from notepad:  on elimination: this should only be relevant to combo-making.
  once a component word is chosen for a particular sentence, all other
  sententce combinations that do not contain that component word should be
  eliminated.
  "So I'll need to keep copies of the per-sentence combination list (list of
  string sets - the sets themselves need not be copied), so that I can remove
  the incompatible combinations on a per-generated-combo basis."
  * this is a potential real optimization I think, for combo-making. Eliminate
    compares altogether vs. elimination via compare.
  * it seems this may be relevant for --xor validation and precompute as well?
    * DONE: should auto-fail if conflicting --xor's are provided
    * what about --or's? do we need to prune --xor's in that case?)
  * need to iron it out more detail above and implement it.
