* have a stream keep track of how many times it's been run, and/or a total
  count of the number of indices it's processed, then total all of them at
  the end, so i show: num streams executed (M), num indices processed (N)

* Hmm the previous (below) comment got me thinking. If a "source" doesn't
  have a variation for a particular sentence, I don't think there's any need
  to compare it to all the xor_sources that also have no variation for that
  sentence, is there? I think I should be able to restrict it to comparing
  only hose sentences *that actually have a variation*. Some legacy clue issue
  emight come to light here though. Maybe I just hardcode it at s < 5 for now.
  * So I tried hardcoding it at s < 5 and the results were wrong. So I'm
    thinking there is some magical coincidental property happening here, where,
    somehow, due to sheer volume of xor_sources i'm comparing against (which
    does involve duplicates), I am effectiveliy comparing against *every*
    xor_source. Seems really weird if that's true, but that's the only
    explanation I have at the moment, and also explains why the below comment
    is true. I think I am just getting lucky that all merged xor.req sources
    contain a lot of s1-s4 sources, and that if I used only an --xor source
    that contained only s5-s9 sources, I wouldn't get any results.
    NOTE: this problem all goes away when I implement the "only check the
    variation with fewest number of sources" (hopeful) optimization, which
    is described 3 paragraphs below.

* It's not at all clear to me how I am handling "legacy" variations at the
  moment, in filter. Like, if a source has a legacy primary clue for a
  particular sentence, am I even calling isXorCompatibleWith on it, with
  anything? This is specific to the variationIndices implementation (I think).
  Somehow I'm getting the same result count both with and without variation
  indices, so it stands to reason I'm doing something right (??). Either that,
  or something wrong in both cases.

* VariationIndices: Explain how there are xor_sources that use variation 0
  (which is really variation -1, i.e. no variation, right?) I think (hope) this
  means "some xor_sources contain no primary source from that sentence."

* VariationIndices: Even though the total number of xor_src variation indices 
  for a particular source, across all sentences, may exceed the total number
  of xor_sources (thus *not* being an optimization), I have a suspicion that
  we can break out of the loop if any set of indices represented by a single
  variation has no matches. For example, if a source uses variation 2 of
  sentence 3, and there are no compatible xor_sources for that variation of
  that sentence, we're done - there is no need to compare xor_sources of
  the variations for other sentences of that source. Ideally, we could sort
  the sentences by "variation indices count", and test them in order of
  smallest to largest. This might just be hopium talking, because I haven't
  fully thought it through, but it's worth testing.
  * Also: if any sentence variations result in 0 matches, we can short circuit
    the search for any other variation matches.
  * So I need to think about this a little more, but I'm now thinking, in my
    late night tired state, that I *only* need to test vs. the xor_sources
    represented by a source's sentence variation with the fewest number of
    indices. There's either a match, or not. Success, or failure. No sorting
    or looping required.
    Update: upon further reflection, I think I need to take the sum of
    variation 0 (sources that contain no clue for that variation), and
    variation N (sources that match that variation). 
    
  * If the above is true, I think we may need to special-case the condition
    where a source has *no variations* across all sentences. In which case,
    we just test vs. every xor source I guess. Probably no sense writing
    lots special code to handle a condition that will go away after all clues
    are converted to sentences.
  * Maybe I could get away with just copying an array of "all incides", e.g.
    from iota, to device memory for this special case.

* markAllCompatibleOrSources in c++.

* Optimizing merge/get_compat_combos. 200M combos currently, and no streams.
  Low reward atm, but eventually might make sense.
  * 200M kernel finishes in ~35ms, which DOES NOT give us enough time to do
    all the things we want, when interleaving streams:
    * copy results from device (~10ms)
    * iterate results and append indices to "hit indices" list. (~135ms)
    * mayyybe do some source-merging on CPU. not sure. very possibly though,
      given that the #of hits is a overall a very small (??ms)
  * it seems like a decent sized chunk of device memory to alloc/copy, esp
    if there are multiple streams (x2, x3 that amount).


* WHEN MERGE EXCEEDS UINT64_T: when I get to this point, which will likely
  result from too many --xor args being specified (although could also happen
  from too many sources of a fewer # of --xor args), I can break down the set
  of --xor args into separate chunks, incremented separately (with separate
  indices). for example, I have an array of 8 matrices, I can make that two
  arrays of 4 matrices. the first array will be 0,0,0,0 for a long time, and
  I can just iterate on the 2nd array until it fills up, then increment first
  array to 0,0,0,1 and process second again. given that I will probably end up
  "chunking" this for kernels anyway due to result set size, I could probably
  chunk-run until array 2 reaches capacity, then "increment" array 1 indices
  on CPU, copy them back over to GPU, and run more chunks. It's not *clear*
  that having an all-GPU solution would be a big win, in other words, due to
  result-set size limitations.

* running multiple list_pair_compat kernels in merge stage seems unnecessary;
  i could make that kernel more loopy and handle all list pairs in one call.
  low priority though, as it finishes the current impl in under 25ms.

* HACK: 35 passed to buildKnownNcSourceListMap, in cm-precompute.ts.
  The problem here is that we have no defined upper-bound on how many primary
  sources a single "clue" or "useSource" could represent. 21 for example,
  which is a rather long one (san juan hill buffalo soldier), goes up to
  30 currently.
  When we call buildAllUseNcDataLists("21"), it is (apparently) unbounded -
  it'll find all occurances of 21.
  When we call buildKnownNcSourceListMap, we could also be unbounded, but
  that would require knowing the longest variation for each sentence, and
  adding them up, I think.
  For practical purposes, 35 seems like a reasonable stopping point at this time.
  However, we really should enforce this in buildAllUseNcDataLists(), otherwise
  we're going to hit the bug again when sourceListMap and allXorNcDataLists
  aren't in sync.

* so it's not the only question that matters, but it seems like a big question,
  why am I passing primaryNameSrcList and ncList of XorSources back to node?
  Sure I use them to generate sourceBits and usedSources, but are they used
  for anything else? I'm probably merging them and shit, but why, what do I
  do with it all when I'm done? I have a feeling, that I am passing the NcLists
  to Native.addCandidate() as well, from where it is *ONLY USED TO GENERATE
  SOURCEBITS* on the add-on side (I think). So is the *ONLY* reason I am
  maintaining ncLists on the node side, is to use them as the "wire protocol"
  for Node<->Addon communication of sourceBits!?
  * Note that this wrap-back-to-JS is disabled now. As a result -t --fast is
    broken, but it's better to fix that another way (or convert to c++) than
    deal with the insanely long wrap times.
  * Note also that since I'm not passing back to JS, the necessity of all the
    ncList/primaryNameSrcList stuff on the C++ side comes into question. I
    suspect it may be needed for something, but I haven't actually confirmed.
    I do know that having to convert a SourceList to a SourceCompatibilityList
    prior to copying to device memory is a consequence I'd like to get rid of.

* tricky situation that should be fixed (and in general, beyond this specific
  tricky instance). blue is a synonym of navy in clues 1.  we later go on to
  derive "navy" as a solution to "dark,blue".  but if the source of the "blue"
  in this case is the synonym of navy, it should not be allowed.

  to achieve this, it feels like we'll have to (optionally? for components.show)
  attach the "source/origin component" to variation sources. e.g. "navy" gets
  attached to that primary synonym "blue", somehow. when --add'ing, check and
  fail if --add'ed word matches any primary "origin" words.

* I may have figured out what that NO_DUPLICATES thing in peco was all
  about. in cm-precompute.ts fillKnownNcSourceListMapForSum, with sum=2
  we apparently iterate over all 1,1 combinations, including duplicates
  like 'newton:1','newton:1' currently. probably should disable that?
  (it's a tiny fraction of combos so probably not a big deal)
  Haha. This may be what's breaking things right now.

* additional thoughts on optimizaiton:
 - only call addCandidate every 1000+ candidates. maintain a list and
   bulk push them.
 - there's some "i didnt do this coz it's 18s out of 18m" comment in
   combo-maker.ts that I could look into. related to using CountBits
   I think.
 - also copyOnMerge logic in mergeAllSources might be worthwhile to
   look into

* candidates.comboListMap should be a vector. It's not even strictly
  necessary anymore.

* NCData/List -> NcListContainer

* output max variation count and max source count (per sentence) at load.
* fail if max sources > 127 for now

* usesourcebits -> implement andCompatible
* eventually bits per sentence could be based on max sources per sentence
  (uniqueNameCount) and usedSources bitset size is the sum, rather than
  being a fixed size (128*9)

* add syn/hom of anagrams at .. post-process?
* do i need a new variation category, "prefixes", for which we also do
  a syn/hom lookup in post-process? e.g. cogburn -> cog -> tooth.
  the alternative is that "cog" is ... what.. an anagram? and thus we
  do a syn lookup? maybe "anagram" could be generalized? kinda nice to
  have each "rule" encapsulated as a separate variation entry type.

* -t full support, including --add, will be necessary (might be working now)

* "addon-mode" module return string "Debug" or "Release" for setting addon path

* so there is a problem with "ignore":true clues which depend on clues in "sentences"
  that don't support the ignore flag. this problem will go away when all clues
  are converted to sentences, presumably, but it's giving incorrect results
  as it stands (hobbit,home appear in valid combinations with xor.req,
  for example, when they should be "used" by "eye").

* -t 20,s or 24,s are valid, shouldn't be. looks like restrictSameClueNumber
  is broke.
  * now it crashes. after putting in support for non-unique legacy clue names
    into the uniquePrimaryClueNames list.
  * so one idea is we could justa call Validator.validateSources(validateAll: false)
    for any supplyed -t (or --xor, --or, etc) combo. If it doesn't pass
    validator, then, duh, it's not valid. The problem with this is if I want
    to support name:count syntax: Validator doesn't (currently) support it.
  * Maybe the crash on bad combos is something I can live with for now.

* NameCount.listHasCompatibleSources looks unnecessary, or weird, or both

* so, ClueManager.getClueList(1) is the culprit. In every case, I need
  to review what I am doing with the list when I call it with 1.

* All of the "getNumPrimarySources()" stuff is a bit weird now isn't it
  I should look into all uses and confirm what it means. And consider
  the "MaxPossiblePrimarySources" case as a possible solution (largest
  sourcelist variation from each sentence).

* validateSources(name) -- enforce everywhere, for (2) below

validatesources is apparently happy with old:3=not:1,not:1,old:1
which is wrong because:
1) FIXED: two nots from two different variations of sentence three. i though i checked
   for and prevented this with the "candidates" thing somewhere
2) TODO: this is like that weird case with "bird" being the "name" of a clue that had
   "bird" as a source, and that should be disallowed.
   
clues3.json:  { "name": "old",             "src": "new,not" },


* OPTIMIZITIONS:

* adding "powerful" to sentence3 components list really caused the number
  of combinations to explode. It's funny that it didn't actually change the
  output, but that's probably due to the xor.req's I used. There is something
  to investigate here and a couple of potential fixes for this:
  0) Investigate: am I ensuring that the sentence words + all components
     form a **unique** list? Should be in Sentence.getUniqueComponentNames
     or something.
  1) I think I can do a 2nd pass of index filtering. Iirc, the order of
     filtering matters, and while I currently do something like "remove items
     from list B that are incompatible with items in list A", I don't do
     the inverse, or something like that, which may help.
     NOTE: I don't think filtering matters right now . It only affects native
     peco-loop timing which is currently fast.
     NOTE: I've looked at this and I think I'm testing every combo, nothing
     further to optimize.
  2) I think there is potential for a filtering step in here somewhere, where,
     for example, I look at all of the NameSrc results for a particular useNC,
     and see what they have in common. I'm just spitballing, but something
     like "well all NcDataCombinations for this NC have *this* and *that*
     source, so lets leave those sourceBits in for the compatibility check,
     but eliminate the words from the combinatorial set, or something.

     - Eh. I don't think i'm on to anything important there either. I mean,
     you either filter *as* you're considering sources to combine, or you
     filter during the combining process *after* you build lists of prospective
     sources to combine (or both). I am not convinced there is anything here
     but probably worth thinking about more.

     - after some more thought, i do think this is worth exploring. probably
     i'll be screwed without some GPU processing, at which point any data
     optimizations will be kind of moot. we'll find out once all clues are
     converted to sentences.


* I think opportunity for optimization is running out in mergecompatible. 
  One last thing to try is somehow having mergecompatLists (and perhaps
  getMergeData) take either a MergedSourcesList or a Source.List. Then
  we'd need a variation of mergeSources took either as the first arg as well.

* so If I really want to speed up combo-maker further, probably I should
  port the whole getCombosForUseNcLists function and dependences (first/next)
  to C++. I don't think just porting mergeCompatibleSources3 and dependencies
  alone will do much as each call is short so the overhead of the call isn't
  worth it, but there are a million (literally) calls.

* Precompute is a tiny bit slower now. Maybe because I'm doing too much
  copying in mergeAllCompatibleSources/XorSources? Any way to put a
  stop to that? Like the optimization idea I might have in a comment there,
  using something like a new CombinedSourceRefs data type.
  Also the "copyOnMerge" logic I use in mergeCompatibleSources3 might be
  of some help here.

** the below was a pre-cuda optimization idea
* from notepad:  on elimination: this should only be relevant to combo-making.
  once a component word is chosen for a particular sentence, all other
  sententce combinations that do not contain that component word should be
  eliminated.
  "So I'll need to keep copies of the per-sentence combination list (list of
  string sets - the sets themselves need not be copied), so that I can remove
  the incompatible combinations on a per-generated-combo basis."
  * this is a potential real optimization I think, for combo-making. Eliminate
    compares altogether vs. elimination via compare.
  * it seems this may be relevant for --xor validation and precompute as well?
    * DONE: should auto-fail if conflicting --xor's are provided
    * what about --or's? do we need to prune --xor's in that case?)
  * need to iron it out more detail above and implement it.
