* we could also store a count (and eventually, list) of  which sources
  match with which xorsource
  
* candidates.comboListMap should be a vector. It's not even strictly
  necessary anymore.

* NCData/List -> NcListContainer

* output max variation count and max source count (per sentence) at load.
* fail if max sources > 127 for now

* usesourcebits -> implement andCompatible
* eventually bits per sentence could be based on max sources per sentence
  (uniqueNameCount) and usedSources bitset size is the sum, rather than
  being a fixed size (128*9)

* add syn/hom of anagrams at .. post-process?
* do i need a new variation category, "prefixes", for which we also do
  a syn/hom lookup in post-process? e.g. cogburn -> cog -> tooth.
  the alternative is that "cog" is ... what.. an anagram? and thus we
  do a syn lookup? maybe "anagram" could be generalized? kinda nice to
  have each "rule" encapsulated as a separate variation entry type.

* -c (and -t?) --or arg support.

* -t --verbose

* -t full support, including --add, will be necessary (might be working now)

* "addon-mode" module return string "Debug" or "Release" for setting addon path

* so there is a problem with "ignore":true clues which depend on clues in "sentences"
  that don't support the ignore flag. this problem will go away when all clues
  are converted to sentences, presumably, but it's giving incorrect results
  as it stands (hobbit,home appear in valid combinations for with xor.req,
  for example, when they should be "used" by "eye").

* -t 20,s or 24,s are valid, shouldn't be. looks like restrictSameClueNumber
  is broke.
  * now it crashes. after putting in support for non-unique legacy clue names
    into the uniquePrimaryClueNames list.
  * so one idea is we could justa call Validator.validateSources(validateAll: false)
    for any supplyed -t (or --xor, --or, etc) combo. If it doesn't pass
    validator, then, duh, it's nto valid. The problem with this is if I want
    to support name:count syntax: Validator doesn't (currently) support it.
  * Maybe the crash on bad combos is something I can live with for now.

* NameCount.listHasCompatibleSources looks unnecessary, or weird, or both

* so, ClueManager.getClueList(1) is the culprit. In every case, I need
  to review what I am doing with the list when I call it with 1.

* All of the "getNumPrimarySources()" stuff is a bit weird now isn't it
  I should look into all uses and confirm what it means. And consider
  the "MaxPossiblePrimarySources" case as a possible solution (largest
  sourcelist variation from each sentence).

* validateSources(name) -- enforce everywhere, for (2) below

validatesources is apparently happy with old:3=not:1,not:1,old:1
which is wrong because:
1) FIXED: two nots from two different variations of sentence three. i though i checked
   for and prevented this with the "candidates" thing somewhere
2) TODO: this is like that weird case with "bird" being the "name" of a clue that had
   "bird" as a source, and that should be disallowed.
   
clues3.json:  { "name": "old",             "src": "new,not" },


* OPTIMIZITIONS:

* adding "powerful" to sentence3 components list really caused the number
  of combinations to explode. It's funny that it didn't actually change the
  output, but that's probably due to the xor.req's I used. There is something
  to investigate here and a couple of potential fixes for this:
  0) Investigate: am I ensuring that the sentence words + all components
     form a **unique** list? Should be in Sentence.getUniqueComponentNames
     or something.
  1) I think I can do a 2nd pass of index filtering. Iirc, the order of
     filtering matters, and while I currently do something like "remove items
     from list B that are incompatible with items in list A", I don't do
     the inverse, or something like that, which may help.
     NOTE: I don't think filtering matters right now . It only affects native
     peco-loop timing which is currently fast.
  2) I think there is potential for a filtering step in here somewhere, where,
     for example, I look at all of the NameSrc results for a particular useNC,
     and see what they have in common. I'm just spitballing, but something
     like "well all NcDataCombinations for this NC have *this* and *that*
     source, so lets leave those sourceBits in for the compatibility check,
     but eliminate the words from the combinatorial set, or something.

     - Eh. I don't think i'm on to anything important there either. I mean,
     you either filter *as* you're considering sources to combine, or you
     filter during the combining process *after* you build lists of prospective
     sources to combine (or both). I am not convinced there is anything here
     but probably worth thinking about more.

     - after some more thought, i do think this is worth exploring. probably
     i'll be screwed without some GPU processing, at which point any data
     optimizations will be kind of moot. we'll find out once all clues are
     converted to sentences.


* I think opportunity for optimization is running out in mergecompatible. 
  One last thing to try is somehow having mergecompatLists (and perhaps
  getMergeData) take either a MergedSourcesList or a Source.List. Then
  we'd need a variation of mergeSources took either as the first arg as well.

* so If I really want to speed up combo-maker further, probably I should
  port the whole getCombosForUseNcLists function and dependences (first/next)
  to C++. I don't think just porting mergeCompatibleSources3 and dependencies
  alone will do much as each call is short so the overhead of the call isn't
  worth it, but there are a million (literally) calls.

  - Actually I'm not so sure about the above. I think the meat of the problem
  is in the native isAnySourceXorCompatible call. Profiler suggests it really
  comes down to the mechanism for compatibility checking between sources. I
  should re-think the bitset approach. 


* Precompute is a tiny bit slower now. Maybe because I'm doing too much
  copying in mergeAllCompatibleSources/XorSources? Any way to put a
  stop to that? Like the optimization idea I might have in a comment there,
  using something like a new CombinedSourceRefs data type.
  Also the "copyOnMerge" logic I use in mergeCompatibleSources3 might be
  of some help here.

** the below was a pre-cuda optimization idea
* from notepad:  on elimination: this should only be relevant to combo-making.
  once a component word is chosen for a particular sentence, all other
  sententce combinations that do not contain that component word should be
  eliminated.
  "So I'll need to keep copies of the per-sentence combination list (list of
  string sets - the sets themselves need not be copied), so that I can remove
  the incompatible combinations on a per-generated-combo basis."
  * this is a potential real optimization I think, for combo-making. Eliminate
    compares altogether vs. elimination via compare.
  * it seems this may be relevant for --xor validation and precompute as well?
    * DONE: should auto-fail if conflicting --xor's are provided
    * what about --or's? do we need to prune --xor's in that case?)
  * need to iron it out more detail above and implement it.
